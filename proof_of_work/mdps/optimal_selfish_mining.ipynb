{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 10e-5\n",
    "T = 70\n",
    "gamma = 0\n",
    "alpha = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the numbers of states is (T+1)*(T+1)*3 because each chain can be up to T length and there are 3 fork states.\n",
    "num_states = (T+1)*(T+1)*3\n",
    "\n",
    "# generate a state to integer mapping and list of states\n",
    "state_mapping = {}\n",
    "states = []\n",
    "count = 0\n",
    "for a in range(T+1):\n",
    "    for h in range(T+1):\n",
    "        for fork in ['irrelevant', 'relevant', 'active']:\n",
    "            state_mapping[(a, h, fork)] = count\n",
    "            states.append((a, h, fork))\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAdoptMatrices(rho):\n",
    "    # creating the adopt transition & reward matrices\n",
    "    adopt_transitions = np.zeros(shape = (num_states, num_states))\n",
    "    adopt_rewards = np.zeros(shape = (num_states, num_states))\n",
    "\n",
    "    # each adopt matrix only can map to (1,0,irrelevant) or (0,1,irrelevant)\n",
    "    adopt_new_state_1_index = state_mapping[(1, 0, 'irrelevant')]\n",
    "    adopt_new_state_2_index = state_mapping[(0, 1, 'irrelevant')]\n",
    "    for state_index in range(num_states):\n",
    "        state = states[state_index]\n",
    "        adopt_transitions[state_index, adopt_new_state_1_index] = alpha\n",
    "        adopt_transitions[state_index, adopt_new_state_2_index] = 1 - alpha\n",
    "        adopt_rewards[state_index, adopt_new_state_1_index] = rho * state[1]\n",
    "        adopt_rewards[state_index, adopt_new_state_2_index] = rho * state[1]\n",
    "\n",
    "    # making matrices sparse\n",
    "    return ss.csr_matrix(adopt_transitions), ss.csr_matrix(adopt_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOverrideMatrices(rho):\n",
    "    # creating the override transition & reward matrices\n",
    "    override_transitions = np.zeros(shape = (num_states, num_states))\n",
    "    override_rewards = np.zeros(shape = (num_states, num_states))\n",
    "\n",
    "    for state_index in range(num_states):\n",
    "        state = states[state_index]\n",
    "        # a > h, which must be true for override to succeed\n",
    "        if state[0] > state[1]:\n",
    "            # (a-h, 0, irrelevant)\n",
    "            new_state_1 = (state[0]-state[1], 0, 'irrelevant')\n",
    "            # (a-h-1, 1, relevant)\n",
    "            new_state_2 = (state[0]-state[1]-1, 1, 'relevant')\n",
    "            override_transitions[state_index, state_mapping[new_state_1]] = alpha\n",
    "            override_transitions[state_index, state_mapping[new_state_2]] = 1 - alpha\n",
    "            override_rewards[state_index, state_mapping[new_state_1]] = (1 - rho) * (state[1] + 1)\n",
    "            override_rewards[state_index, state_mapping[new_state_2]] = (1 - rho) * (state[1] + 1)\n",
    "        else:\n",
    "            # filling in remainder of array.\n",
    "            override_transitions[state_index, 0] = 1\n",
    "            override_rewards[state_index, 0] = -1*rho*1000\n",
    "\n",
    "    # making matrices sparse\n",
    "    return ss.csr_matrix(override_transitions), ss.csr_matrix(override_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWaitMatrices(rho):\n",
    "    # creating the wait transition & reward matrices\n",
    "    wait_transitions = np.zeros(shape = (num_states, num_states))\n",
    "    wait_rewards = np.zeros(shape = (num_states, num_states))\n",
    "\n",
    "    for state_index in range(num_states):\n",
    "        state = states[state_index]\n",
    "        # ignore truncated states\n",
    "        if (state[0] == T or state[1] == T):\n",
    "            ### FIX THIS\n",
    "            wait_transitions[state_index, 0] = 1\n",
    "            continue\n",
    "\n",
    "        # irrelevant or relevant\n",
    "        if (state[2] == 'irrelevant') or (state[2] == 'relevant'):\n",
    "            # (a+1, h, irrelevant)\n",
    "            new_state_1 = (state[0] + 1, state[1], 'irrelevant')\n",
    "            # (a, h+1, relevant)\n",
    "            new_state_2 = (state[0], state[1] + 1, 'relevant')\n",
    "            wait_transitions[state_index, state_mapping[new_state_1]] = alpha\n",
    "            wait_transitions[state_index, state_mapping[new_state_2]] = 1 - alpha\n",
    "        # active\n",
    "        else:\n",
    "            # a >= h\n",
    "            if state[0] >= state[1]: \n",
    "                # (a+1, h, active)\n",
    "                new_state_1 = (state[0] + 1, state[1], 'active')\n",
    "                # (a-h, 1, relevant)\n",
    "                new_state_2 = (state[0] - state[1], 1, 'relevant')\n",
    "                # (a, h+1, relevant)\n",
    "                new_state_3 = (state[0], state[1] + 1, 'relevant')\n",
    "                wait_transitions[state_index, state_mapping[new_state_1]] = alpha\n",
    "                wait_transitions[state_index, state_mapping[new_state_2]] = gamma * (1 - alpha)\n",
    "                wait_transitions[state_index, state_mapping[new_state_3]] = (1 - gamma) * (1 - alpha)\n",
    "                wait_rewards[state_index, state_mapping[new_state_2]] = (1 - rho) * state[1]\n",
    "            else:\n",
    "                wait_transitions[state_index, 0] = 1\n",
    "                wait_rewards[state_index, 0] = -1*rho*1000\n",
    "\n",
    "    # making matrices sparse\n",
    "    return ss.csr_matrix(wait_transitions), ss.csr_matrix(wait_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatchMatrices(rho):\n",
    "    # creating the match transition & rewards matrices\n",
    "    match_transitions = np.zeros(shape = (num_states, num_states))\n",
    "    match_rewards = np.zeros(shape = (num_states, num_states))\n",
    "\n",
    "    for state_index in range(num_states):\n",
    "        state = states[state_index]\n",
    "        # ignore truncated states\n",
    "        if (state[0] == T or state[1] == T):\n",
    "            #### FIX THIS\n",
    "            match_transitions[state_index, 0] = 1\n",
    "            continue\n",
    "\n",
    "\n",
    "        # a >= h and relevant\n",
    "        if (state[0] >= state[1]) and (state[2] == 'relevant'):\n",
    "            # (a+1, h, active)\n",
    "            new_state_1 = (state[0] + 1, state[1], 'active')\n",
    "            # (a-h, 1, relevant)\n",
    "            new_state_2 = (state[0] - state[1], 1, 'relevant')\n",
    "            # (a, h+1, relevant)\n",
    "            new_state_3 = (state[0], state[1] + 1, 'relevant')\n",
    "            match_transitions[state_index, state_mapping[new_state_1]] = alpha\n",
    "            match_transitions[state_index, state_mapping[new_state_2]] = gamma * (1 - alpha)\n",
    "            match_transitions[state_index, state_mapping[new_state_3]] = (1 - gamma) * (1 - alpha)\n",
    "            match_rewards[state_index, state_mapping[new_state_2]] = (1 - rho) * state[1]\n",
    "        else:\n",
    "            match_transitions[state_index, 0] = 1\n",
    "            match_rewards[state_index, 0] = -1*rho*1000\n",
    "\n",
    "    # making matrices sparse\n",
    "    return ss.csr_matrix(match_transitions), ss.csr_matrix(match_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix = np.asarray([adopt_transitions, override_transitions, wait_transitions, match_transitions])\n",
    "reward_matrix = np.asarray([adopt_rewards, override_rewards, wait_rewards, match_rewards])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvi = mdptoolbox.mdp.RelativeValueIteration(transition_matrix, reward_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvi.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rvi.average_reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
