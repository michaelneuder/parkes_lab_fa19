{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as ss\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ss.SparseEfficiencyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numOfStates:  15123\n"
     ]
    }
   ],
   "source": [
    "maxForkLen = 70\n",
    "numOfStates = (maxForkLen+1) * (maxForkLen+1) * 3\n",
    "print('numOfStates: ', numOfStates)\n",
    "alphaPower = 0.3\n",
    "gammaRatio = 0\n",
    "irrelevant = 0; relevant = 1; active = 2;\n",
    "choices = 4\n",
    "adopt = 0; override = 1; match = 2; wait = 3;\n",
    "P = []; Rs = []; Rh = [];\n",
    "for _ in range(choices):\n",
    "    P.append(ss.csr_matrix(np.zeros(shape=(numOfStates, numOfStates))))\n",
    "    Rs.append(ss.csr_matrix(np.zeros(shape=(numOfStates, numOfStates))))\n",
    "    Rh.append(ss.csr_matrix(np.zeros(shape=(numOfStates, numOfStates))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a state to integer mapping and list of states\n",
    "state_mapping = {}\n",
    "states = []\n",
    "count = 0\n",
    "for a in range(maxForkLen+1):\n",
    "    for h in range(maxForkLen+1):\n",
    "        for fork in range(3):\n",
    "            state_mapping[(a, h, fork)] = count\n",
    "            states.append((a, h, fork))\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing state 0\n",
      "processing state 2000\n",
      "processing state 4000\n",
      "processing state 6000\n",
      "processing state 8000\n",
      "processing state 10000\n",
      "processing state 12000\n",
      "processing state 14000\n"
     ]
    }
   ],
   "source": [
    "# adopt\n",
    "P[adopt][:, state_mapping[1, 0, irrelevant]] = alphaPower\n",
    "P[adopt][:, state_mapping[0, 1, irrelevant]] = 1 - alphaPower\n",
    "for state_index in range(numOfStates):\n",
    "    if state_index % 2000 == 0:\n",
    "        print('processing state', state_index)\n",
    "    a, h, fork = states[state_index]\n",
    "    \n",
    "    # adopt rewards\n",
    "    Rh[adopt][state_index, state_mapping[1, 0, irrelevant]] = h\n",
    "    Rh[adopt][state_index, state_mapping[0, 1, irrelevant]] = h\n",
    "    \n",
    "    # override\n",
    "    if a > h:\n",
    "        P[override][state_index, state_mapping[a-h, 0, irrelevant]] = alphaPower\n",
    "        Rs[override][state_index, state_mapping[a-h, 0, irrelevant]] = h+1\n",
    "        P[override][state_index, state_mapping[a-h-1, 1, relevant]] = 1 - alphaPower\n",
    "        Rs[override][state_index, state_mapping[a-h-1, 1, relevant]] = h+1\n",
    "    else:\n",
    "        P[override][state_index, 0] = 1\n",
    "        Rh[override][state_index, 0] = 10000\n",
    "        \n",
    "    # wait\n",
    "    if (fork != active) and (a < maxForkLen) and (h < maxForkLen):\n",
    "        P[wait][state_index, state_mapping[a+1, h, irrelevant]] = alphaPower\n",
    "        P[wait][state_index, state_mapping[a, h+1, relevant]] = 1 - alphaPower\n",
    "    elif (fork == active) and (a > h) and (h > 0) and (a < maxForkLen) and (h < maxForkLen): \n",
    "        P[wait][state_index, state_mapping[a+1, h, active]] = alphaPower\n",
    "        P[wait][state_index, state_mapping[a-h, 1, relevant]] = gammaRatio*(1-alphaPower)\n",
    "        Rs[wait][state_index, state_mapping[a-h, 1, relevant]] = h\n",
    "        P[wait][state_index, state_mapping[a, h+1, relevant]] = (1-gammaRatio)*(1-alphaPower)\n",
    "    else:\n",
    "        P[wait][state_index, 0] = 1\n",
    "        Rh[wait][state_index, 0] = 10000\n",
    "    \n",
    "    # match\n",
    "    if (fork == relevant) and (a >= h) and (h > 0) and (a < maxForkLen) and (h < maxForkLen):\n",
    "        P[match][state_index, state_mapping[a+1, h, active]] = alphaPower\n",
    "        P[match][state_index, state_mapping[a-h, 1, relevant]] = gammaRatio*(1-alphaPower)\n",
    "        Rs[match][state_index, state_mapping[a-h, 1, relevant]] = h\n",
    "        P[match][state_index, state_mapping[a, h+1, relevant]] = (1-gammaRatio)*(1-alphaPower)\n",
    "    else:\n",
    "        P[match][state_index, 0] = 1\n",
    "        Rh[match][state_index, 0] = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 1 0\n",
      "0.25 0.5 0\n",
      "0.375 0.5 0.25\n",
      "0.3125 0.375 0.25\n",
      "0.28125 0.3125 0.25\n",
      "0.296875 0.3125 0.28125\n",
      "0.3046875 0.3125 0.296875\n",
      "0.30078125 0.3046875 0.296875\n",
      "0.298828125 0.30078125 0.296875\n",
      "0.2998046875 0.30078125 0.298828125\n",
      "0.30029296875 0.30078125 0.2998046875\n",
      "0.300048828125 0.30029296875 0.2998046875\n",
      "0.2999267578125 0.300048828125 0.2998046875\n",
      "0.29998779296875 0.300048828125 0.2999267578125\n",
      "0.300018310546875 0.300048828125 0.29998779296875\n",
      "0.3000030517578125 0.300018310546875 0.29998779296875\n",
      "0.29999542236328125 0.3000030517578125 0.29998779296875\n",
      "lower bound reward: 0.29999542236328125\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.0001\n",
    "lowRho = 0\n",
    "highRho = 1\n",
    "while(highRho - lowRho > epsilon/8):\n",
    "    rho = (highRho + lowRho) / 2;\n",
    "    print(rho, highRho, lowRho)\n",
    "    Wrho = []\n",
    "    for i in range(choices):\n",
    "        Wrho.append((1-rho)*Rs[i] - rho*Rh[i])\n",
    "    rvi = mdptoolbox.mdp.RelativeValueIteration(P, Wrho, epsilon/8)\n",
    "    rvi.run()\n",
    "    lowerBoundPolicy = rvi.policy\n",
    "    reward = rvi.average_reward\n",
    "    if reward > 0:\n",
    "        lowRho = rho\n",
    "    else:\n",
    "        highRho = rho\n",
    "print('lower bound reward:', rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5499954223632812 0.5999954223632812 0.4999954223632812\n",
      "0.5249954223632812 0.5499954223632812 0.4999954223632812\n",
      "0.5124954223632812 0.5249954223632812 0.4999954223632812\n",
      "0.5062454223632812 0.5124954223632812 0.4999954223632812\n",
      "0.5031204223632812 0.5062454223632812 0.4999954223632812\n",
      "0.5015579223632812 0.5031204223632812 0.4999954223632812\n",
      "0.5007766723632812 0.5015579223632812 0.4999954223632812\n",
      "0.5003860473632812 0.5007766723632812 0.4999954223632812\n",
      "0.5001907348632812 0.5003860473632812 0.4999954223632812\n",
      "0.5000930786132812 0.5001907348632812 0.4999954223632812\n",
      "0.5000442504882812 0.5000930786132812 0.4999954223632812\n",
      "0.5000198364257812 0.5000442504882812 0.4999954223632812\n",
      "0.5000076293945312 0.5000198364257812 0.4999954223632812\n",
      "upper bound reward 0.5000076293945312\n"
     ]
    }
   ],
   "source": [
    "lowerBoundRho = rho\n",
    "lowRho = rho\n",
    "highRho = min(rho+0.1, 1)\n",
    "while (highRho - lowRho) > (epsilon / 8):\n",
    "    rho = (highRho + lowRho) / 2\n",
    "    print(rho, highRho, lowRho)\n",
    "    for state_index in range(numOfStates):\n",
    "        a, h, fork = states[state_index]\n",
    "        if a == maxForkLen:\n",
    "            expr = (1-rho)*alphaPower*(1-alphaPower)/(1-2*alphaPower)**2+0.5*((a-h)/(1-2*alphaPower)+a+h)\n",
    "            Rs[adopt][state_index, state_mapping[1, 0, irrelevant]] = expr\n",
    "            Rs[adopt][state_index, state_mapping[0, 1, irrelevant]] = expr\n",
    "            Rs[adopt][state_index, state_mapping[1, 0, irrelevant]] = 0\n",
    "            Rs[adopt][state_index, state_mapping[0, 1, irrelevant]] = 0\n",
    "        elif h == maxForkLen:\n",
    "            expr1 = (1 - np.power(alphaPower/(1-alphaPower), h - a)) * (-1*rho*h)\n",
    "            expr2 = np.power(alphaPower/(1-alphaPower), h - a) * (1 - rho)\n",
    "            expr3 = (alphaPower * (1-alphaPower)) / (np.power(1-2*alphaPower, 2)) + (h - a) / (1- 2 * alphaPower)\n",
    "            expr_total = expr1 + expr2 * expr3\n",
    "            Rs[adopt][state_index, state_mapping[1, 0, irrelevant]] = expr_total\n",
    "            Rs[adopt][state_index, state_mapping[0, 1, irrelevant]] = expr_total\n",
    "            Rh[adopt][state_index, state_mapping[1, 0, irrelevant]] = 0\n",
    "            Rh[adopt][state_index, state_mapping[0, 1, irrelevant]] = 0\n",
    "    Wrho = []\n",
    "    for i in range(choices):\n",
    "        Wrho.append((1-rho)*Rs[i] - rho*Rh[i])\n",
    "    rhoPrime = max(lowRho - epsilon/4, 0)\n",
    "    rvi = mdptoolbox.mdp.RelativeValueIteration(P, Wrho, epsilon/8)\n",
    "    rvi.run()\n",
    "    reward = rvi.average_reward\n",
    "    policy = rvi.policy\n",
    "    if reward > 0:\n",
    "        lowRho = rho\n",
    "    else:\n",
    "        highRho = rho\n",
    "print('upper bound reward', rho)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
