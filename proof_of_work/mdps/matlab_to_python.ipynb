{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numOfStates:  17328\n"
     ]
    }
   ],
   "source": [
    "maxForkLen = 75\n",
    "numOfStates = (maxForkLen+1) * (maxForkLen+1) * 3\n",
    "print('numOfStates: ', numOfStates)\n",
    "alphaPower = 0.45\n",
    "gammaRatio = 0\n",
    "irrelevant = 0; relevant = 1; active = 2;\n",
    "choices = 4\n",
    "adopt = 0; override = 1; match = 2; wait = 3;\n",
    "P = []; Rs = []; Rh = [];\n",
    "for _ in range(choices):\n",
    "    P.append(ss.csr_matrix(np.zeros(shape=(numOfStates, numOfStates))))\n",
    "    Rs.append(ss.csr_matrix(np.zeros(shape=(numOfStates, numOfStates))))\n",
    "    Rh.append(ss.csr_matrix(np.zeros(shape=(numOfStates, numOfStates))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a state to integer mapping and list of states\n",
    "state_mapping = {}\n",
    "states = []\n",
    "count = 0\n",
    "for a in range(maxForkLen+1):\n",
    "    for h in range(maxForkLen+1):\n",
    "        for fork in range(3):\n",
    "            state_mapping[(a, h, fork)] = count\n",
    "            states.append((a, h, fork))\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing state 0\n",
      "processing state 2000\n",
      "processing state 4000\n",
      "processing state 6000\n",
      "processing state 8000\n",
      "processing state 10000\n",
      "processing state 12000\n",
      "processing state 14000\n",
      "processing state 16000\n"
     ]
    }
   ],
   "source": [
    "# adopt\n",
    "P[adopt][:, state_mapping[1, 0, irrelevant]] = alphaPower\n",
    "P[adopt][:, state_mapping[0, 1, irrelevant]] = 1 - alphaPower\n",
    "for state_index in range(numOfStates):\n",
    "    if state_index % 2000 == 0:\n",
    "        print('processing state', state_index)\n",
    "    a, h, fork = states[state_index]\n",
    "    \n",
    "    # adopt rewards\n",
    "    Rh[adopt][state_index, state_mapping[1, 0, irrelevant]] = h\n",
    "    Rh[adopt][state_index, state_mapping[0, 1, irrelevant]] = h\n",
    "    \n",
    "    # override\n",
    "    if a > h:\n",
    "        P[override][state_index, state_mapping[a-h, 0, irrelevant]] = alphaPower\n",
    "        Rs[override][state_index, state_mapping[a-h, 0, irrelevant]] = h+1\n",
    "        P[override][state_index, state_mapping[a-h-1, 1, relevant]] = 1 - alphaPower\n",
    "        Rs[override][state_index, state_mapping[a-h-1, 1, relevant]] = h+1\n",
    "    else:\n",
    "        P[override][state_index, 0] = 1\n",
    "        Rh[override][state_index, 0] = 10000\n",
    "        \n",
    "    # wait\n",
    "    if (fork != active) and (a < maxForkLen) and (h < maxForkLen):\n",
    "        P[wait][state_index, state_mapping[a+1, h, irrelevant]] = alphaPower\n",
    "        P[wait][state_index, state_mapping[a, h+1, relevant]] = 1 - alphaPower\n",
    "    elif (fork == active) and (a > h) and (h > 0) and (a < maxForkLen) and (h < maxForkLen): \n",
    "        P[wait][state_index, state_mapping[a+1, h, active]] = alphaPower\n",
    "        P[wait][state_index, state_mapping[a-h, 1, relevant]] = gammaRatio*(1-alphaPower)\n",
    "        Rs[wait][state_index, state_mapping[a-h, 1, relevant]] = h\n",
    "        P[wait][state_index, state_mapping[a, h+1, relevant]] = (1-gammaRatio)*(1-alphaPower)\n",
    "    else:\n",
    "        P[wait][state_index, 0] = 1\n",
    "        Rh[wait][state_index, 0] = 10000\n",
    "    \n",
    "    # match\n",
    "    if (fork == relevant) and (a >= h) and (h > 0) and (a < maxForkLen) and (h < maxForkLen):\n",
    "        P[match][state_index, state_mapping[a+1, h, active]] = alphaPower\n",
    "        P[match][state_index, state_mapping[a-h, 1, relevant]] = gammaRatio*(1-alphaPower)\n",
    "        Rs[match][state_index, state_mapping[a-h, 1, relevant]] = h\n",
    "        P[match][state_index, state_mapping[a, h+1, relevant]] = (1-gammaRatio)*(1-alphaPower)\n",
    "    else:\n",
    "        P[match][state_index, 0] = 1\n",
    "        Rh[match][state_index, 0] = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 1 0\n",
      "0.75 1 0.5\n",
      "0.625 0.75 0.5\n",
      "0.6875 0.75 0.625\n",
      "0.65625 0.6875 0.625\n",
      "0.671875 0.6875 0.65625\n",
      "0.6640625 0.671875 0.65625\n",
      "0.66796875 0.671875 0.6640625\n",
      "0.669921875 0.671875 0.66796875\n",
      "0.6689453125 0.669921875 0.66796875\n",
      "0.66845703125 0.6689453125 0.66796875\n",
      "0.668212890625 0.66845703125 0.66796875\n",
      "0.6680908203125 0.668212890625 0.66796875\n",
      "0.66815185546875 0.668212890625 0.6680908203125\n",
      "0.668121337890625 0.66815185546875 0.6680908203125\n",
      "0.6681365966796875 0.66815185546875 0.668121337890625\n",
      "0.6681442260742188 0.66815185546875 0.6681365966796875\n",
      "lower bound reward: 0.6681442260742188\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.0001\n",
    "lowRho = 0\n",
    "highRho = 1\n",
    "while(highRho - lowRho > epsilon/8):\n",
    "    rho = (highRho + lowRho) / 2;\n",
    "    print(rho, highRho, lowRho)\n",
    "    Wrho = []\n",
    "    for i in range(choices):\n",
    "        Wrho.append((1-rho)*Rs[i] - rho*Rh[i])\n",
    "    rvi = mdptoolbox.mdp.RelativeValueIteration(P, Wrho, epsilon/8)\n",
    "    rvi.run()\n",
    "    lowerBoundPolicy = rvi.policy\n",
    "    reward = rvi.average_reward\n",
    "    if reward > 0:\n",
    "        lowRho = rho\n",
    "    else:\n",
    "        highRho = rho\n",
    "print('lower bound reward:', rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7181442260742188 0.7681442260742187 0.6681442260742188\n",
      "0.6931442260742188 0.7181442260742188 0.6681442260742188\n",
      "0.6806442260742187 0.6931442260742188 0.6681442260742188\n",
      "0.6743942260742187 0.6806442260742187 0.6681442260742188\n",
      "0.6712692260742188 0.6743942260742187 0.6681442260742188\n",
      "0.6697067260742188 0.6712692260742188 0.6681442260742188\n",
      "0.6704879760742188 0.6712692260742188 0.6697067260742188\n",
      "0.6708786010742188 0.6712692260742188 0.6704879760742188\n",
      "0.6706832885742189 0.6708786010742188 0.6704879760742188\n",
      "0.6705856323242189 0.6706832885742189 0.6704879760742188\n",
      "0.6705368041992188 0.6705856323242189 0.6704879760742188\n",
      "0.6705612182617189 0.6705856323242189 0.6705368041992188\n",
      "0.6705490112304688 0.6705612182617189 0.6705368041992188\n",
      "upper bound reward 0.6705490112304688\n"
     ]
    }
   ],
   "source": [
    "lowerBoundRho = rho\n",
    "lowRho = rho\n",
    "highRho = min(rho+0.1, 1)\n",
    "while (highRho - lowRho) > (epsilon / 8):\n",
    "    rho = (highRho + lowRho) / 2\n",
    "    print(rho, highRho, lowRho)\n",
    "    for state_index in range(numOfStates):\n",
    "        a, h, fork = states[state_index]\n",
    "        if a == maxForkLen:\n",
    "            expr = (1-rho)*alphaPower*(1-alphaPower)/(1-2*alphaPower)**2+0.5*((a-h)/(1-2*alphaPower)+a+h)\n",
    "            Rs[adopt][state_index, state_mapping[1, 0, irrelevant]] = expr\n",
    "            Rs[adopt][state_index, state_mapping[0, 1, irrelevant]] = expr\n",
    "            Rs[adopt][state_index, state_mapping[1, 0, irrelevant]] = 0\n",
    "            Rs[adopt][state_index, state_mapping[0, 1, irrelevant]] = 0\n",
    "        elif h == maxForkLen:\n",
    "            expr1 = (1 - np.power(alphaPower/(1-alphaPower), h - a)) * (-1*rho*h)\n",
    "            expr2 = np.power(alphaPower/(1-alphaPower), h - a) * (1 - rho)\n",
    "            expr3 = (alphaPower * (1-alphaPower)) / (np.power(1-2*alphaPower, 2)) + (h - a) / (1- 2 * alphaPower)\n",
    "            expr_total = expr1 + expr2 * expr3\n",
    "            Rs[adopt][state_index, state_mapping[1, 0, irrelevant]] = expr_total\n",
    "            Rs[adopt][state_index, state_mapping[0, 1, irrelevant]] = expr_total\n",
    "            Rh[adopt][state_index, state_mapping[1, 0, irrelevant]] = 0\n",
    "            Rh[adopt][state_index, state_mapping[0, 1, irrelevant]] = 0\n",
    "    Wrho = []\n",
    "    for i in range(choices):\n",
    "        Wrho.append((1-rho)*Rs[i] - rho*Rh[i])\n",
    "    rhoPrime = max(lowRho - epsilon/4, 0)\n",
    "    rvi = mdptoolbox.mdp.RelativeValueIteration(P, Wrho, epsilon/8)\n",
    "    rvi.run()\n",
    "    reward = rvi.average_reward\n",
    "    policy = rvi.policy\n",
    "    if reward > 0:\n",
    "        lowRho = rho\n",
    "    else:\n",
    "        highRho = rho\n",
    "print('upper bound reward', rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha:  0.05\n",
      "processing state 0\n",
      "processing state 2000\n",
      "processing state 4000\n",
      "processing state 6000\n",
      "processing state 8000\n",
      "processing state 10000\n",
      "processing state 12000\n",
      "processing state 14000\n",
      "processing state 16000\n",
      "lower bound reward: 0.04999542236328125\n",
      "upper bound reward 0.05000762939453125\n",
      "alpha:  0.1\n",
      "processing state 0\n",
      "processing state 2000\n",
      "processing state 4000\n",
      "processing state 6000\n",
      "processing state 8000\n",
      "processing state 10000\n",
      "processing state 12000\n",
      "processing state 14000\n",
      "processing state 16000\n",
      "lower bound reward: 0.09999847412109375\n",
      "upper bound reward 0.10001068115234375\n",
      "alpha:  0.15000000000000002\n",
      "processing state 0\n",
      "processing state 2000\n",
      "processing state 4000\n",
      "processing state 6000\n",
      "processing state 8000\n",
      "processing state 10000\n",
      "processing state 12000\n",
      "processing state 14000\n",
      "processing state 16000\n",
      "lower bound reward: 0.15000152587890625\n",
      "upper bound reward 0.15001373291015624\n",
      "alpha:  0.2\n",
      "processing state 0\n",
      "processing state 2000\n",
      "processing state 4000\n",
      "processing state 6000\n",
      "processing state 8000\n",
      "processing state 10000\n",
      "processing state 12000\n",
      "processing state 14000\n",
      "processing state 16000\n",
      "lower bound reward: 0.20000457763671875\n",
      "upper bound reward 0.20001678466796874\n",
      "alpha:  0.25\n",
      "processing state 0\n",
      "processing state 2000\n",
      "processing state 4000\n",
      "processing state 6000\n",
      "processing state 8000\n",
      "processing state 10000\n",
      "processing state 12000\n",
      "processing state 14000\n",
      "processing state 16000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ea883b632bf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mWrho\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mRs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mRh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mrvi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdptoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRelativeValueIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0mrvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mlowerBoundPolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/mdptoolbox/mdp.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, transitions, reward, epsilon, max_iter)\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0;31m# Initialise a relative value iteration MDP.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m         \u001b[0mMDP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtransitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/mdptoolbox/mdp.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, transitions, reward, discount, epsilon, max_iter)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;31m# we run a check on P and R to make sure they are describing an MDP. If\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;31m# an exception isn't raised then they are assumed to be correct.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0m_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_computeDimensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_computeTransition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/mdptoolbox/util.py\u001b[0m in \u001b[0;36mcheck\u001b[0;34m(P, R)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;31m# Check that the P's are square, stochastic and non-negative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0maa\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mcheckSquareStochastic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maa\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetSpan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/mdptoolbox/util.py\u001b[0m in \u001b[0;36mcheckSquareStochastic\u001b[0;34m(matrix)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misSquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0m_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSquareError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misStochastic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0m_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStochasticError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misNonNegative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/mdptoolbox/util.py\u001b[0m in \u001b[0;36misStochastic\u001b[0;34m(matrix)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \"\"\"\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mabsdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_np\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0m_np\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mmatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_np\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for alpha in np.linspace(0.05, 0.45, 9):\n",
    "    maxForkLen = 75\n",
    "    numOfStates = (maxForkLen+1) * (maxForkLen+1) * 3\n",
    "    print('alpha: ', alpha)\n",
    "    alphaPower = alpha\n",
    "    gammaRatio = 0\n",
    "    irrelevant = 0; relevant = 1; active = 2;\n",
    "    choices = 4\n",
    "    adopt = 0; override = 1; match = 2; wait = 3;\n",
    "    P = []; Rs = []; Rh = [];\n",
    "    for _ in range(choices):\n",
    "        P.append(ss.csr_matrix(np.zeros(shape=(numOfStates, numOfStates))))\n",
    "        Rs.append(ss.csr_matrix(np.zeros(shape=(numOfStates, numOfStates))))\n",
    "        Rh.append(ss.csr_matrix(np.zeros(shape=(numOfStates, numOfStates))))\n",
    "    # generate a state to integer mapping and list of states\n",
    "    state_mapping = {}\n",
    "    states = []\n",
    "    count = 0\n",
    "    for a in range(maxForkLen+1):\n",
    "        for h in range(maxForkLen+1):\n",
    "            for fork in range(3):\n",
    "                state_mapping[(a, h, fork)] = count\n",
    "                states.append((a, h, fork))\n",
    "                count += 1\n",
    "    # adopt\n",
    "    P[adopt][:, state_mapping[1, 0, irrelevant]] = alphaPower\n",
    "    P[adopt][:, state_mapping[0, 1, irrelevant]] = 1 - alphaPower\n",
    "    for state_index in range(numOfStates):\n",
    "        if state_index % 2000 == 0:\n",
    "            print('processing state', state_index)\n",
    "        a, h, fork = states[state_index]\n",
    "\n",
    "        # adopt rewards\n",
    "        Rh[adopt][state_index, state_mapping[1, 0, irrelevant]] = h\n",
    "        Rh[adopt][state_index, state_mapping[0, 1, irrelevant]] = h\n",
    "\n",
    "        # override\n",
    "        if a > h:\n",
    "            P[override][state_index, state_mapping[a-h, 0, irrelevant]] = alphaPower\n",
    "            Rs[override][state_index, state_mapping[a-h, 0, irrelevant]] = h+1\n",
    "            P[override][state_index, state_mapping[a-h-1, 1, relevant]] = 1 - alphaPower\n",
    "            Rs[override][state_index, state_mapping[a-h-1, 1, relevant]] = h+1\n",
    "        else:\n",
    "            P[override][state_index, 0] = 1\n",
    "            Rh[override][state_index, 0] = 10000\n",
    "\n",
    "        # wait\n",
    "        if (fork != active) and (a < maxForkLen) and (h < maxForkLen):\n",
    "            P[wait][state_index, state_mapping[a+1, h, irrelevant]] = alphaPower\n",
    "            P[wait][state_index, state_mapping[a, h+1, relevant]] = 1 - alphaPower\n",
    "        elif (fork == active) and (a > h) and (h > 0) and (a < maxForkLen) and (h < maxForkLen): \n",
    "            P[wait][state_index, state_mapping[a+1, h, active]] = alphaPower\n",
    "            P[wait][state_index, state_mapping[a-h, 1, relevant]] = gammaRatio*(1-alphaPower)\n",
    "            Rs[wait][state_index, state_mapping[a-h, 1, relevant]] = h\n",
    "            P[wait][state_index, state_mapping[a, h+1, relevant]] = (1-gammaRatio)*(1-alphaPower)\n",
    "        else:\n",
    "            P[wait][state_index, 0] = 1\n",
    "            Rh[wait][state_index, 0] = 10000\n",
    "\n",
    "        # match\n",
    "        if (fork == relevant) and (a >= h) and (h > 0) and (a < maxForkLen) and (h < maxForkLen):\n",
    "            P[match][state_index, state_mapping[a+1, h, active]] = alphaPower\n",
    "            P[match][state_index, state_mapping[a-h, 1, relevant]] = gammaRatio*(1-alphaPower)\n",
    "            Rs[match][state_index, state_mapping[a-h, 1, relevant]] = h\n",
    "            P[match][state_index, state_mapping[a, h+1, relevant]] = (1-gammaRatio)*(1-alphaPower)\n",
    "        else:\n",
    "            P[match][state_index, 0] = 1\n",
    "            Rh[match][state_index, 0] = 10000\n",
    "    epsilon = 0.0001\n",
    "    lowRho = 0\n",
    "    highRho = 1\n",
    "    while(highRho - lowRho > epsilon/8):\n",
    "        rho = (highRho + lowRho) / 2;\n",
    "        Wrho = []\n",
    "        for i in range(choices):\n",
    "            Wrho.append((1-rho)*Rs[i] - rho*Rh[i])\n",
    "        rvi = mdptoolbox.mdp.RelativeValueIteration(P, Wrho, epsilon/8)\n",
    "        rvi.run()\n",
    "        lowerBoundPolicy = rvi.policy\n",
    "        reward = rvi.average_reward\n",
    "        if reward > 0:\n",
    "            lowRho = rho\n",
    "        else:\n",
    "            highRho = rho\n",
    "    print('lower bound reward:', rho)\n",
    "    lowerBoundRho = rho\n",
    "    lowRho = rho\n",
    "    highRho = min(rho+0.1, 1)\n",
    "    while (highRho - lowRho) > (epsilon / 8):\n",
    "        rho = (highRho + lowRho) / 2\n",
    "        for state_index in range(numOfStates):\n",
    "            a, h, fork = states[state_index]\n",
    "            if a == maxForkLen:\n",
    "                expr = (1-rho)*alphaPower*(1-alphaPower)/(1-2*alphaPower)**2+0.5*((a-h)/(1-2*alphaPower)+a+h)\n",
    "                Rs[adopt][state_index, state_mapping[1, 0, irrelevant]] = expr\n",
    "                Rs[adopt][state_index, state_mapping[0, 1, irrelevant]] = expr\n",
    "                Rs[adopt][state_index, state_mapping[1, 0, irrelevant]] = 0\n",
    "                Rs[adopt][state_index, state_mapping[0, 1, irrelevant]] = 0\n",
    "            elif h == maxForkLen:\n",
    "                expr1 = (1 - np.power(alphaPower/(1-alphaPower), h - a)) * (-1*rho*h)\n",
    "                expr2 = np.power(alphaPower/(1-alphaPower), h - a) * (1 - rho)\n",
    "                expr3 = (alphaPower * (1-alphaPower)) / (np.power(1-2*alphaPower, 2)) + (h - a) / (1- 2 * alphaPower)\n",
    "                expr_total = expr1 + expr2 * expr3\n",
    "                Rs[adopt][state_index, state_mapping[1, 0, irrelevant]] = expr_total\n",
    "                Rs[adopt][state_index, state_mapping[0, 1, irrelevant]] = expr_total\n",
    "                Rh[adopt][state_index, state_mapping[1, 0, irrelevant]] = 0\n",
    "                Rh[adopt][state_index, state_mapping[0, 1, irrelevant]] = 0\n",
    "        Wrho = []\n",
    "        for i in range(choices):\n",
    "            Wrho.append((1-rho)*Rs[i] - rho*Rh[i])\n",
    "        rhoPrime = max(lowRho - epsilon/4, 0)\n",
    "        rvi = mdptoolbox.mdp.RelativeValueIteration(P, Wrho, epsilon/8)\n",
    "        rvi.run()\n",
    "        reward = rvi.average_reward\n",
    "        policy = rvi.policy\n",
    "        if reward > 0:\n",
    "            lowRho = rho\n",
    "        else:\n",
    "            highRho = rho\n",
    "    print('upper bound reward', rho)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
