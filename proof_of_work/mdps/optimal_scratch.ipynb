{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAdoptMatrices(rho, underpaying):\n",
    "    # creating the adopt transition & reward matrices\n",
    "    adopt_transitions = np.zeros(shape = (num_states, num_states))\n",
    "    adopt_rewards = np.zeros(shape = (num_states, num_states))\n",
    "\n",
    "    # each adopt matrix only can map to (1,0,irrelevant) or (0,1,irrelevant)\n",
    "    new_state_1 = (1, 0, 'irrelevant')\n",
    "    new_state_2 = (0, 1, 'irrelevant')\n",
    "    for state_index in range(num_states):\n",
    "        a, h, fork = states[state_index]\n",
    "        adopt_transitions[state_index, state_mapping[new_state_1]] = alpha\n",
    "        adopt_transitions[state_index, state_mapping[new_state_2]] = 1 - alpha\n",
    "        adopt_rewards[state_index, state_mapping[new_state_2]] = -1 * rho * h\n",
    "        adopt_rewards[state_index, state_mapping[new_state_2]] = -1 * rho * h\n",
    "    \n",
    "    adjustAdopt(adopt_transitions, adopt_rewards, rho, underpaying)\n",
    "    # making matrices sparse\n",
    "    return ss.csr_matrix(adopt_transitions), ss.csr_matrix(adopt_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOverrideMatrices(rho):\n",
    "    # creating the override transition & reward matrices\n",
    "    override_transitions = np.zeros(shape = (num_states, num_states))\n",
    "    override_rewards = np.zeros(shape = (num_states, num_states))\n",
    "\n",
    "    for state_index in range(num_states):\n",
    "        a, h, fork = states[state_index]\n",
    "        if a > h:\n",
    "            new_state_1 = (a - h, 0, 'irrelevant')\n",
    "            new_state_2 = (a - h - 1, 1, 'relevant')\n",
    "            override_transitions[state_index, state_mapping[new_state_1]] = alpha\n",
    "            override_transitions[state_index, state_mapping[new_state_2]] = 1 - alpha\n",
    "            override_rewards[state_index, state_mapping[new_state_1]] = (1 - rho) * (h + 1)\n",
    "            override_rewards[state_index, state_mapping[new_state_2]] = (1 - rho) * (h + 1)\n",
    "        else:\n",
    "            # filling in remainder of array.\n",
    "            override_transitions[state_index, 0] = 1\n",
    "            override_rewards[state_index, 0] = -1 * rho * 10000\n",
    "    \n",
    "    forceAdopt(override_transitions, override_rewards, rho)\n",
    "    return ss.csr_matrix(override_transitions), ss.csr_matrix(override_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWaitMatrices(rho):\n",
    "    # creating the wait transition & reward matrices\n",
    "    wait_transitions = np.zeros(shape = (num_states, num_states))\n",
    "    wait_rewards = np.zeros(shape = (num_states, num_states))\n",
    "\n",
    "    for state_index in range(num_states):\n",
    "        a, h, fork = states[state_index]\n",
    "        # irrelevant or relevant\n",
    "        if ((fork == 'irrelevant') or (fork == 'relevant')) and (a < T) and (h < T):\n",
    "            new_state_1 = (a + 1, h, 'irrelevant')\n",
    "            new_state_2 = (a, h + 1, 'relevant')\n",
    "            wait_transitions[state_index, state_mapping[new_state_1]] = alpha\n",
    "            wait_transitions[state_index, state_mapping[new_state_2]] = 1 - alpha\n",
    "        # active\n",
    "        elif (fork == 'active') and (a < T) and (h < T):\n",
    "            if a >= h: \n",
    "                new_state_1 = (a + 1, h, 'active')\n",
    "                new_state_2 = (a - h, 1, 'relevant')\n",
    "                new_state_3 = (a, h + 1, 'relevant')\n",
    "                wait_transitions[state_index, state_mapping[new_state_1]] = alpha\n",
    "                wait_transitions[state_index, state_mapping[new_state_2]] = gamma * (1 - alpha)\n",
    "                wait_transitions[state_index, state_mapping[new_state_3]] = (1 - gamma) * (1 - alpha)\n",
    "                wait_rewards[state_index, state_mapping[new_state_2]] = (1 - rho) * h\n",
    "            else:\n",
    "                wait_transitions[state_index, 0] = 1\n",
    "                wait_rewards[state_index, 0] = -1 * rho * 10000\n",
    "        else:\n",
    "            wait_transitions[state_index, 0] = 1\n",
    "            wait_rewards[state_index, 0] = -1 * rho * 10000\n",
    "\n",
    "    forceAdopt(wait_transitions, wait_rewards, rho)\n",
    "    return ss.csr_matrix(wait_transitions), ss.csr_matrix(wait_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatchMatrices(rho):\n",
    "    # creating the match transition & rewards matrices\n",
    "    match_transitions = np.zeros(shape = (num_states, num_states))\n",
    "    match_rewards = np.zeros(shape = (num_states, num_states))\n",
    "\n",
    "    for state_index in range(num_states):\n",
    "        a, h, fork = states[state_index]\n",
    "        if (a >= h) and (fork == 'relevant') and (a < T) and (h < T):\n",
    "            new_state_1 = (a + 1, h, 'active')\n",
    "            new_state_2 = (a - h, 1, 'relevant')\n",
    "            new_state_3 = (a, h + 1, 'relevant')\n",
    "            match_transitions[state_index, state_mapping[new_state_1]] = alpha\n",
    "            match_transitions[state_index, state_mapping[new_state_2]] = gamma * (1 - alpha)\n",
    "            match_transitions[state_index, state_mapping[new_state_3]] = (1 - gamma) * (1 - alpha)\n",
    "            match_rewards[state_index, state_mapping[new_state_2]] = (1 - rho) * h\n",
    "        else:\n",
    "            match_transitions[state_index, 0] = 1\n",
    "            match_rewards[state_index, 0] = -1 * rho * 10000\n",
    "\n",
    "    forceAdopt(match_transitions, match_rewards, rho)\n",
    "    return ss.csr_matrix(match_transitions), ss.csr_matrix(match_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjustAdopt(transition_matrix, reward_matrix, rho, underpaying):\n",
    "    new_state_1_index = state_mapping[(1, 0, 'irrelevant')]\n",
    "    new_state_2_index = state_mapping[(0, 1, 'irrelevant')]\n",
    "    for state_index in range(num_states):\n",
    "        a, h, fork = states[state_index]\n",
    "        if ((a == T) or (h == T)) and (h != a):\n",
    "            # clear out old probabilities\n",
    "            transition_matrix[state_index, :] = 0\n",
    "            transition_matrix[state_index, new_state_1_index] = alpha\n",
    "            transition_matrix[state_index, new_state_2_index] = 1 - alpha\n",
    "            if underpaying:\n",
    "                reward_matrix[state_index, new_state_1_index] = -1 * rho * h\n",
    "                reward_matrix[state_index, new_state_2_index] = -1 * rho * h\n",
    "            else:\n",
    "                # attacker ahead\n",
    "                if a > h:\n",
    "                    reward_matrix[state_index, new_state_1_index] = (1-rho) * overpayAttackerAhead(a, h, rho)\n",
    "                    reward_matrix[state_index, new_state_2_index] = (1-rho) * overpayAttackerAhead(a, h, rho)\n",
    "                else:\n",
    "                    reward_matrix[state_index, new_state_1_index] = (1-rho) * overpayHonestAhead(a, h, rho)\n",
    "                    reward_matrix[state_index, new_state_2_index] = (1-rho) * overpayHonestAhead(a, h, rho)\n",
    "\n",
    "def forceAdopt(transition_matrix, reward_matrix, rho):\n",
    "    for state_index in range(num_states):\n",
    "        a, h, fork = states[state_index]\n",
    "        if ((a == T) or (h == T)):\n",
    "            # clear out old probabilities\n",
    "            transition_matrix[state_index, :] = 0\n",
    "            transition_matrix[state_index, 0] = 1\n",
    "            reward_matrix[state_index, 0] = -1 * rho * 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def overpayAttackerAhead(a, h, rho):\n",
    "    expr1 = (1 - rho) * (alpha * (1 - alpha)) / ((1 - 2 * alpha)**2)\n",
    "    expr2 = (1/2) * (((a - h) / (1 - 2 * alpha)) + a + h)\n",
    "    return expr1 + expr2\n",
    "\n",
    "def overpayHonestAhead(a, h, rho):\n",
    "    expr1 = (1 - np.power(alpha/(1-alpha), h - a)) * (-1*rho*h)\n",
    "    expr2 = np.power(alpha/(1-alpha), h - a) * (1 - rho)\n",
    "    expr3 = (alpha * (1-alpha)) / (np.power(1-2*alpha, 2)) + (h - a) / (1-2*alpha)\n",
    "    return expr1 + expr2 * expr3\n",
    "\n",
    "def getAllMatrices(rho, underpaying):\n",
    "    adopt_t, adopt_r = getAdoptMatrices(rho, underpaying)\n",
    "    override_t, override_r = getOverrideMatrices(rho)\n",
    "    wait_t, wait_r = getWaitMatrices(rho)\n",
    "    match_t, match_r = getMatchMatrices(rho)\n",
    "    return [adopt_t, override_t, wait_t, match_t], [adopt_r, override_r, wait_r, match_r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 5\n",
    "\n",
    "# the numbers of states is (T+1)*(T+1)*3 because each chain can be up to T length and there are 3 fork states.\n",
    "# num_states = (T+1)*(T+1)*3\n",
    "\n",
    "# generate a state to integer mapping and list of states\n",
    "state_mapping = {}\n",
    "states = []\n",
    "count = 0\n",
    "for a in range(T+1):\n",
    "    for h in range(T+1):\n",
    "        for fork in ['irrelevant', 'relevant', 'active']:\n",
    "            if fork == 'relevant' and h == 0:\n",
    "                continue\n",
    "            elif fork == 'irrelevant' and a == 0:\n",
    "                continue\n",
    "            state_mapping[(a, h, fork)] = count\n",
    "            states.append((a, h, fork))\n",
    "            count += 1\n",
    "\n",
    "state_mapping[(0, 1, 'irrelevant')] = count\n",
    "states.append((0, 1, 'irrelevant'))\n",
    "num_states = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing params\n",
    "epsilon = 10e-5\n",
    "gamma = 0\n",
    "alpha = 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 0.5\n",
      "0.5 1 0.75\n",
      "0.5 0.75 0.625\n",
      "0.625 0.75 0.6875\n",
      "0.625 0.6875 0.65625\n",
      "0.65625 0.6875 0.671875\n",
      "0.671875 0.6875 0.6796875\n",
      "0.671875 0.6796875 0.67578125\n",
      "0.671875 0.67578125 0.673828125\n",
      "0.671875 0.673828125 0.6728515625\n",
      "0.671875 0.6728515625 0.67236328125\n",
      "0.67236328125 0.6728515625 0.672607421875\n",
      "0.672607421875 0.6728515625 0.6727294921875\n",
      "0.672607421875 0.6727294921875 0.67266845703125\n",
      "0.672607421875 0.67266845703125 0.672637939453125\n",
      "0.672637939453125 0.67266845703125 0.6726531982421875\n",
      "0.672637939453125 0.6726531982421875 0.6726455688476562\n",
      "0.45\n",
      "upper bound:  1.9005237977857337\n",
      "lower bound:  0.6725455688476563\n"
     ]
    }
   ],
   "source": [
    "# main algo\n",
    "low = 0; high = 1\n",
    "while (high - low) > epsilon / 8:\n",
    "    rho = (low + high) / 2\n",
    "    print(low, high, rho)\n",
    "    transitions, rewards = getAllMatrices(rho, underpaying=True)\n",
    "    rvi = mdptoolbox.mdp.RelativeValueIteration(transitions, rewards, epsilon/8)\n",
    "    rvi.run()\n",
    "    if rvi.average_reward > 0:\n",
    "        low = rho\n",
    "    else:\n",
    "        high = rho\n",
    "lower_bound = rho - epsilon\n",
    "rho_prime = np.max(low - epsilon/4, 0)\n",
    "transitions, rewards = getAllMatrices(rho_prime, underpaying=False)\n",
    "rvi = mdptoolbox.mdp.RelativeValueIteration(transitions, rewards, epsilon)\n",
    "rvi.run()\n",
    "upper_bound = rho_prime + 2 * (rvi.average_reward + epsilon)\n",
    "print(alpha)\n",
    "print(\"upper bound: \", upper_bound)\n",
    "print(\"lower bound: \", lower_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upper bound:  0.7739318652956764\n",
      "lower bound:  0.7852927612304688\n"
     ]
    }
   ],
   "source": [
    "rho = 0.7853927612304688\n",
    "low = 0.7853851318359375\n",
    "lower_bound = rho - epsilon\n",
    "rho_prime = np.max(low - epsilon/4, 0)\n",
    "transitions, rewards = getAllMatrices(rho_prime, underpaying=True)\n",
    "rvi = mdptoolbox.mdp.RelativeValueIteration(transitions, rewards, epsilon)\n",
    "rvi.run()\n",
    "upper_bound = rho_prime + 2 * (rvi.average_reward + epsilon)\n",
    "print(\"upper bound: \", upper_bound)\n",
    "print(\"lower bound: \", lower_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 2, 'irrelevant'), 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = state_mapping[(4,2,'irrelevant')]\n",
    "states[index], rvi.policy[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32686855675173965"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rvi.average_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 1 0.5\n",
    "# 0.5 1 0.75\n",
    "# 0.75 1 0.875\n",
    "# 0.75 0.875 0.8125\n",
    "# 0.75 0.8125 0.78125\n",
    "# 0.78125 0.8125 0.796875\n",
    "# 0.78125 0.796875 0.7890625\n",
    "# 0.78125 0.7890625 0.78515625\n",
    "# 0.78515625 0.7890625 0.787109375\n",
    "# 0.78515625 0.787109375 0.7861328125\n",
    "# 0.78515625 0.7861328125 0.78564453125\n",
    "# 0.78515625 0.78564453125 0.785400390625\n",
    "# 0.78515625 0.785400390625 0.7852783203125\n",
    "# 0.7852783203125 0.785400390625 0.78533935546875\n",
    "# 0.78533935546875 0.785400390625 0.785369873046875\n",
    "# 0.785369873046875 0.785400390625 0.7853851318359375\n",
    "# 0.7853851318359375 0.785400390625 0.7853927612304688\n",
    "# 0.45\n",
    "# upper bound:  0.9358516523294126\n",
    "# lower bound:  0.7852927612304688"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
