{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAdoptMatrices(rho):\n",
    "    # creating the adopt transition & reward matrices\n",
    "    adopt_transitions = np.zeros(shape = (num_states, num_states))\n",
    "    adopt_rewards = np.zeros(shape = (num_states, num_states))\n",
    "\n",
    "    # each adopt matrix only can map to (1,0,irrelevant) or (0,1,irrelevant)\n",
    "    new_state_1 = (1, 0, 'irrelevant')\n",
    "    new_state_2 = (0, 1, 'irrelevant')\n",
    "    for state_index in range(num_states):\n",
    "        a, h, fork = states[state_index]\n",
    "        adopt_transitions[state_index, state_mapping[new_state_1]] = alpha\n",
    "        adopt_transitions[state_index, state_mapping[new_state_2]] = 1 - alpha\n",
    "        adopt_rewards[state_index, state_mapping[new_state_2]] = -1 * rho * h\n",
    "        adopt_rewards[state_index, state_mapping[new_state_2]] = -1 * rho * h\n",
    "        \n",
    "    # making matrices sparse\n",
    "    return adopt_transitions, adopt_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOverrideMatrices(rho):\n",
    "    # creating the override transition & reward matrices\n",
    "    override_transitions = np.zeros(shape = (num_states, num_states))\n",
    "    override_rewards = np.zeros(shape = (num_states, num_states))\n",
    "\n",
    "    for state_index in range(num_states):\n",
    "        a, h, fork = states[state_index]\n",
    "        # a > h, which must be true for override to succeed\n",
    "        if a > h:\n",
    "            new_state_1 = (a - h, 0, 'irrelevant')\n",
    "            new_state_2 = (a - h - 1, 1, 'relevant')\n",
    "            override_transitions[state_index, state_mapping[new_state_1]] = alpha\n",
    "            override_transitions[state_index, state_mapping[new_state_2]] = 1 - alpha\n",
    "            override_rewards[state_index, state_mapping[new_state_1]] = (1 - rho) * (h + 1)\n",
    "            override_rewards[state_index, state_mapping[new_state_2]] = (1 - rho) * (h + 1)\n",
    "        else:\n",
    "            # filling in remainder of array.\n",
    "            override_transitions[state_index, 0] = 1\n",
    "            override_rewards[state_index, 0] = -1 * rho * 1000\n",
    "\n",
    "    # making matrices sparse\n",
    "    return override_transitions, override_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWaitMatrices(rho):\n",
    "    # creating the wait transition & reward matrices\n",
    "    wait_transitions = np.zeros(shape = (num_states, num_states))\n",
    "    wait_rewards = np.zeros(shape = (num_states, num_states))\n",
    "\n",
    "    for state_index in range(num_states):\n",
    "        a, h, fork = states[state_index]\n",
    "        # irrelevant or relevant\n",
    "        if ((fork == 'irrelevant') or (fork == 'relevant')) and (a < T) and (h < T):\n",
    "            new_state_1 = (a + 1, h, 'irrelevant')\n",
    "            new_state_2 = (a, h + 1, 'relevant')\n",
    "            wait_transitions[state_index, state_mapping[new_state_1]] = alpha\n",
    "            wait_transitions[state_index, state_mapping[new_state_2]] = 1 - alpha\n",
    "        # active\n",
    "        elif (fork == 'active') and (a < T) and (h < T) and (h > 0):\n",
    "            if a >= h: \n",
    "                new_state_1 = (a + 1, h, 'active')\n",
    "                new_state_2 = (a - h, 1, 'relevant')\n",
    "                new_state_3 = (a, h + 1, 'relevant')\n",
    "                wait_transitions[state_index, state_mapping[new_state_1]] = alpha\n",
    "                wait_transitions[state_index, state_mapping[new_state_2]] = gamma * (1 - alpha)\n",
    "                wait_transitions[state_index, state_mapping[new_state_3]] = (1 - gamma) * (1 - alpha)\n",
    "                wait_rewards[state_index, state_mapping[new_state_2]] = (1 - rho) * h\n",
    "            else:\n",
    "                wait_transitions[state_index, 0] = 1\n",
    "                wait_rewards[state_index, 0] = -1 * rho * 1000\n",
    "        else:\n",
    "            wait_transitions[state_index, 0] = 1\n",
    "            wait_rewards[state_index, 0] = -1 * rho * 1000\n",
    "\n",
    "    # making matrices sparse\n",
    "    return wait_transitions, wait_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatchMatrices(rho):\n",
    "    # creating the match transition & rewards matrices\n",
    "    match_transitions = np.zeros(shape = (num_states, num_states))\n",
    "    match_rewards = np.zeros(shape = (num_states, num_states))\n",
    "\n",
    "    for state_index in range(num_states):\n",
    "        a, h, fork = states[state_index]\n",
    "        if (a >= h) and (fork == 'relevant') and (a < T) and (h < T) and (h > 0):\n",
    "            new_state_1 = (a + 1, h, 'active')\n",
    "            new_state_2 = (a - h, 1, 'relevant')\n",
    "            new_state_3 = (a, h + 1, 'relevant')\n",
    "            match_transitions[state_index, state_mapping[new_state_1]] = alpha\n",
    "            match_transitions[state_index, state_mapping[new_state_2]] = gamma * (1 - alpha)\n",
    "            match_transitions[state_index, state_mapping[new_state_3]] = (1 - gamma) * (1 - alpha)\n",
    "            match_rewards[state_index, state_mapping[new_state_2]] = (1 - rho) * h\n",
    "        else:\n",
    "            match_transitions[state_index, 0] = 1\n",
    "            match_rewards[state_index, 0] = -1 * rho * 1000\n",
    "\n",
    "    # making matrices sparse\n",
    "    return match_transitions, match_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forceAdopt(transition_matrix, reward_matrix, rho, underpaying):\n",
    "    new_state_1_index = state_mapping[(1, 0, 'irrelevant')]\n",
    "    new_state_2_index = state_mapping[(0, 1, 'irrelevant')]\n",
    "    for state_index in range(num_states):\n",
    "        a, h, fork = states[state_index]\n",
    "        if ((a == T) or (h == T)) and (a != h):\n",
    "            # clear out old probabilities\n",
    "            transition_matrix[state_index, :] = 0\n",
    "            transition_matrix[state_index, new_state_1_index] = alpha\n",
    "            transition_matrix[state_index, new_state_2_index] = 1 - alpha\n",
    "            if underpaying:\n",
    "                reward_matrix[state_index, new_state_1_index] = -1 * rho * h\n",
    "                reward_matrix[state_index, new_state_2_index] = -1 * rho * h\n",
    "            else:\n",
    "                # attacker ahead\n",
    "                if a > h:\n",
    "                    reward_matrix[state_index, new_state_1_index] = overpayAttackerAhead(a, h, rho)\n",
    "                    reward_matrix[state_index, new_state_2_index] = overpayAttackerAhead(a, h, rho)\n",
    "                else:\n",
    "                    reward_matrix[state_index, new_state_1_index] = overpayHonestAhead(a, h, rho)\n",
    "                    reward_matrix[state_index, new_state_2_index] = overpayHonestAhead(a, h, rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "def overpayAttackerAhead(a, h, rho):\n",
    "    assert(a > h)\n",
    "    expr1 = (1 - rho) * (alpha * (1 - alpha)) / ((1 - 2 * alpha)**2)\n",
    "    expr2 = (1/2) * ((a - h) / (1 - 2 * alpha) + a + h)\n",
    "    return expr1 + expr2\n",
    "\n",
    "def overpayHonestAhead(a, h, rho):\n",
    "    assert(h > a)\n",
    "    expr1 = (1 - np.power(alpha/(1-alpha), h - a)) * (-1*rho*h)\n",
    "    expr2 = np.power(alpha/(1-alpha), h - a) * (1 - rho)\n",
    "    expr3 = (alpha * (1-alpha)) / (np.power(1-2*alpha, 2)) + (h - a) / (1- 2 * alpha)\n",
    "    return expr1 + expr2 * expr3\n",
    "\n",
    "def getAllMatrices(rho, underpaying=True):\n",
    "    adopt_t, adopt_r = getAdoptMatrices(rho)\n",
    "    forceAdopt(adopt_t, adopt_r, rho, underpaying)\n",
    "    adopt_t = ss.csr_matrix(adopt_t); adopt_r = ss.csr_matrix(adopt_r)\n",
    "    override_t, override_r = getOverrideMatrices(rho)\n",
    "    forceAdopt(override_t, override_r, rho, underpaying)\n",
    "    override_t = ss.csr_matrix(override_t); override_r = ss.csr_matrix(override_r)\n",
    "    wait_t, wait_r = getWaitMatrices(rho)\n",
    "    forceAdopt(wait_t, wait_r, rho, underpaying)\n",
    "    wait_t = ss.csr_matrix(wait_t); wait_r = ss.csr_matrix(wait_r)\n",
    "    match_t, match_r = getMatchMatrices(rho)\n",
    "    forceAdopt(match_t, match_r, rho, underpaying)\n",
    "    match_t = ss.csr_matrix(match_t); match_r = ss.csr_matrix(match_r)\n",
    "    return [adopt_t, override_t, wait_t, match_t], [adopt_r, override_r, wait_r, match_r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the numbers of states is (T+1)*(T+1)*3 because each chain can be up to T length and there are 3 fork states.\n",
    "num_states = (T+1)*(T+1)*3\n",
    "\n",
    "# generate a state to integer mapping and list of states\n",
    "state_mapping = {}\n",
    "states = []\n",
    "count = 0\n",
    "for a in range(T+1):\n",
    "    for h in range(T+1):\n",
    "        for fork in ['irrelevant', 'relevant', 'active']:\n",
    "            state_mapping[(a, h, fork)] = count\n",
    "            states.append((a, h, fork))\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing params\n",
    "epsilon = 10e-5\n",
    "T = 75\n",
    "gamma = 0\n",
    "alpha = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 0.5\n",
      "0 0.5 0.25\n",
      "0 0.25 0.125\n",
      "0.125 0.25 0.1875\n",
      "0.1875 0.25 0.21875\n",
      "0.21875 0.25 0.234375\n",
      "0.234375 0.25 0.2421875\n",
      "0.234375 0.2421875 0.23828125\n",
      "0.234375 0.23828125 0.236328125\n",
      "0.236328125 0.23828125 0.2373046875\n",
      "0.2373046875 0.23828125 0.23779296875\n",
      "0.23779296875 0.23828125 0.238037109375\n",
      "0.238037109375 0.23828125 0.2381591796875\n",
      "0.238037109375 0.2381591796875 0.23809814453125\n",
      "0.238037109375 0.23809814453125 0.238067626953125\n",
      "0.238067626953125 0.23809814453125 0.2380828857421875\n",
      "0.2380828857421875 0.23809814453125 0.23809051513671875\n"
     ]
    }
   ],
   "source": [
    "# main alg\n",
    "low = 0; high = 1\n",
    "while (high - low) >= epsilon / 8:\n",
    "    rho = (low + high) / 2\n",
    "    print(low, high, rho)\n",
    "    transitions, rewards = getAllMatrices(rho, underpaying=True)\n",
    "    rvi = mdptoolbox.mdp.RelativeValueIteration(transitions, rewards, epsilon/8)\n",
    "    rvi.run()\n",
    "    if rvi.average_reward > 0:\n",
    "        low = rho\n",
    "    else:\n",
    "        high = rho\n",
    "lower_bound = rho - epsilon\n",
    "rho_prime = np.max(low - epsilon/4, 0)\n",
    "transitions, rewards = getAllMatrices(rho_prime, underpaying=False)\n",
    "rvi = mdptoolbox.mdp.RelativeValueIteration(transitions, rewards, epsilon)\n",
    "rvi.run()\n",
    "upper_bound = rho_prime + 2 * (rvi.average_reward + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.23799051513671876, 0.23831544970703208)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAdoptMatrices(rho, underpaying=True):\n",
    "    # creating the adopt transition & reward matrices\n",
    "    adopt_transitions = np.zeros(shape = (num_states, num_states))\n",
    "    adopt_rewards = np.zeros(shape = (num_states, num_states))\n",
    "\n",
    "    # each adopt matrix only can map to (1,0,irrelevant) or (0,1,irrelevant)\n",
    "    new_state_1 = (1, 0, 'irrelevant')\n",
    "    new_state_2 = (0, 1, 'irrelevant')\n",
    "    for state_index in range(num_states):\n",
    "        a, h, fork = states[state_index]\n",
    "        adopt_transitions[state_index, state_mapping[new_state_1]] = alpha\n",
    "        adopt_transitions[state_index, state_mapping[new_state_2]] = 1 - alpha\n",
    "        adopt_rewards[state_index, state_mapping[new_state_2]] = rho * h\n",
    "        adopt_rewards[state_index, state_mapping[new_state_2]] = rho * h\n",
    "        if ((a == T) or (h == T)) and (a != h):\n",
    "            # overpaying\n",
    "            if not underpaying:\n",
    "                # attacker ahead\n",
    "                if a > h: \n",
    "                    adopt_rewards[state_index, state_mapping[new_state_1]] = overpayAttackerAhead(a, h, rho)\n",
    "                    adopt_rewards[state_index, state_mapping[new_state_2]] = overpayAttackerAhead(a, h, rho)\n",
    "                # honest ahead\n",
    "                else: \n",
    "                    adopt_rewards[state_index, state_mapping[new_state_1]] = overpayHonestAhead(a, h, rho)\n",
    "                    adopt_rewards[state_index, state_mapping[new_state_2]] = overpayHonestAhead(a, h, rho)\n",
    "    \n",
    "    # making matrices sparse\n",
    "    return ss.csr_matrix(adopt_transitions), ss.csr_matrix(adopt_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-dde0cfbc6a8c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-dde0cfbc6a8c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    0 1 0.5\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "0 1 0.5\n",
    "0.5 1 0.75\n",
    "0.5 0.75 0.625\n",
    "0.5 0.625 0.5625\n",
    "0.5625 0.625 0.59375\n",
    "0.59375 0.625 0.609375\n",
    "0.609375 0.625 0.6171875\n",
    "0.609375 0.6171875 0.61328125\n",
    "0.61328125 0.6171875 0.615234375\n",
    "0.61328125 0.615234375 0.6142578125\n",
    "0.6142578125 0.615234375 0.61474609375\n",
    "0.6142578125 0.61474609375 0.614501953125\n",
    "0.6142578125 0.614501953125 0.6143798828125\n",
    "0.6142578125 0.6143798828125 0.61431884765625\n",
    "0.6142578125 0.61431884765625 0.614288330078125\n",
    "0.6142578125 0.614288330078125 0.6142730712890625\n",
    "0.6142730712890625 0.614288330078125 0.6142807006835938\n",
    "0.0017484939746807981"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
