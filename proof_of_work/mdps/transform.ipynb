{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as ss\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=ss.SparseEfficiencyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init params\n",
    "alpha = 0.45\n",
    "T = 5\n",
    "state_count = (T+1) * (T+1) * 3\n",
    "gamma = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDP helpers\n",
    "irrelevant = 0; relevant = 1; active = 2\n",
    "choices = 4\n",
    "adopt = 0; override = 1; match = 2; wait = 3\n",
    "# generate a state to integer mapping and list of states\n",
    "state_mapping = {}\n",
    "states = []\n",
    "count = 0\n",
    "for a in range(T+1):\n",
    "    for h in range(T+1):\n",
    "        for fork in range(3):\n",
    "            state_mapping[(a, h, fork)] = count\n",
    "            states.append((a, h, fork))\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing state 0\n"
     ]
    }
   ],
   "source": [
    "# transition and reward matrices\n",
    "transitions = []; reward_selfish = []; reward_honest = []\n",
    "for _ in range(choices):\n",
    "    transitions.append(ss.csr_matrix(np.zeros(shape=(state_count, state_count))))\n",
    "    reward_selfish.append(ss.csr_matrix(np.zeros(shape=(state_count, state_count))))\n",
    "    reward_honest.append(ss.csr_matrix(np.zeros(shape=(state_count, state_count))))\n",
    "\n",
    "# writing transition and reward data \n",
    "for state_index in range(state_count):\n",
    "    if state_index % 2000 == 0:\n",
    "        print('processing state', state_index)\n",
    "    \n",
    "    a, h, fork = states[state_index]\n",
    "    \n",
    "    # adopt transitions\n",
    "    transitions[adopt][state_index, state_mapping[1, 0, irrelevant]] = alpha\n",
    "    transitions[adopt][state_index, state_mapping[0, 1, irrelevant]] = 1 - alpha\n",
    "    # adopt rewards\n",
    "    reward_honest[adopt][state_index, state_mapping[1, 0, irrelevant]] = h\n",
    "    reward_honest[adopt][state_index, state_mapping[0, 1, irrelevant]] = h\n",
    "\n",
    "    # override\n",
    "    if a > h:\n",
    "        transitions[override][state_index, state_mapping[a-h, 0, irrelevant]] = alpha\n",
    "        reward_selfish[override][state_index, state_mapping[a-h, 0, irrelevant]] = h+1\n",
    "        transitions[override][state_index, state_mapping[a-h-1, 1, relevant]] = 1 - alpha\n",
    "        reward_selfish[override][state_index, state_mapping[a-h-1, 1, relevant]] = h+1\n",
    "    else:\n",
    "        transitions[override][state_index, 0] = 1\n",
    "        reward_honest[override][state_index, 0] = 10000\n",
    "\n",
    "    # wait\n",
    "    if (fork != active) and (a < T) and (h < T):\n",
    "        transitions[wait][state_index, state_mapping[a+1, h, irrelevant]] = alpha\n",
    "        transitions[wait][state_index, state_mapping[a, h+1, relevant]] = 1 - alpha\n",
    "    elif (fork == active) and (a > h) and (h > 0) and (a < T) and (h < T): \n",
    "        transitions[wait][state_index, state_mapping[a+1, h, active]] = alpha\n",
    "        transitions[wait][state_index, state_mapping[a-h, 1, relevant]] = gamma*(1-alpha)\n",
    "        reward_selfish[wait][state_index, state_mapping[a-h, 1, relevant]] = h\n",
    "        transitions[wait][state_index, state_mapping[a, h+1, relevant]] = (1-gamma)*(1-alpha)\n",
    "    else:\n",
    "        transitions[wait][state_index, 0] = 1\n",
    "        reward_honest[wait][state_index, 0] = 10000\n",
    "\n",
    "    # match\n",
    "    if (fork == relevant) and (a >= h) and (h > 0) and (a < T) and (h < T):\n",
    "        transitions[match][state_index, state_mapping[a+1, h, active]] = alpha\n",
    "        transitions[match][state_index, state_mapping[a-h, 1, relevant]] = gamma*(1-alpha)\n",
    "        reward_selfish[match][state_index, state_mapping[a-h, 1, relevant]] = h\n",
    "        transitions[match][state_index, state_mapping[a, h+1, relevant]] = (1-gamma)*(1-alpha)\n",
    "    else:\n",
    "        transitions[match][state_index, 0] = 1\n",
    "        reward_honest[match][state_index, 0] = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha:  0.45 lower bound reward: 0.5507888793945312\n",
      "alpha:  0.45 upper bound reward 0.6507766723632813\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.0001\n",
    "lowRho = 0\n",
    "highRho = 1\n",
    "while(highRho - lowRho > epsilon/8):\n",
    "    rho = (highRho + lowRho) / 2;\n",
    "    Wrho = []\n",
    "    for i in range(choices):\n",
    "        Wrho.append((1-rho)*reward_selfish[i] - rho*reward_honest[i])\n",
    "    rvi = mdptoolbox.mdp.RelativeValueIteration(transitions, Wrho, epsilon/8)\n",
    "    rvi.run()\n",
    "    lowerBoundPolicy = rvi.policy\n",
    "    reward = rvi.average_reward\n",
    "    if reward > 0:\n",
    "        lowRho = rho\n",
    "    else:\n",
    "        highRho = rho\n",
    "print('alpha: ', alpha, 'lower bound reward:', rho)\n",
    "lowerBoundRho = rho\n",
    "lowRho = rho\n",
    "highRho = min(rho+0.1, 1)\n",
    "while (highRho - lowRho) > (epsilon / 8):\n",
    "    rho = (highRho + lowRho) / 2\n",
    "    for state_index in range(state_count):\n",
    "        a, h, fork = states[state_index]\n",
    "        if a == T:\n",
    "            expr = (1-rho)*alpha*(1-alpha)/(1-2*alpha)**2+0.5*((a-h)/(1-2*alpha)+a+h)\n",
    "            reward_selfish[adopt][state_index, state_mapping[1, 0, irrelevant]] = expr\n",
    "            reward_selfish[adopt][state_index, state_mapping[0, 1, irrelevant]] = expr\n",
    "            reward_honest[adopt][state_index, state_mapping[1, 0, irrelevant]] = 0\n",
    "            reward_honest[adopt][state_index, state_mapping[0, 1, irrelevant]] = 0\n",
    "        elif h == T:\n",
    "            expr1 = (1 - np.power(alpha/(1-alpha), h - a)) * (-1*rho*h)\n",
    "            expr2 = np.power(alpha/(1-alpha), h - a) * (1 - rho)\n",
    "            expr3 = (alpha * (1-alpha)) / (np.power(1-2*alpha, 2)) + (h - a) / (1- 2 * alpha)\n",
    "            expr_total = expr1 + expr2 * expr3\n",
    "            reward_selfish[adopt][state_index, state_mapping[1, 0, irrelevant]] = expr_total\n",
    "            reward_selfish[adopt][state_index, state_mapping[0, 1, irrelevant]] = expr_total\n",
    "            reward_honest[adopt][state_index, state_mapping[1, 0, irrelevant]] = 0\n",
    "            reward_honest[adopt][state_index, state_mapping[0, 1, irrelevant]] = 0\n",
    "    Wrho = []\n",
    "    for i in range(choices):\n",
    "        Wrho.append((1-rho)*reward_selfish[i] - rho*reward_honest[i])\n",
    "    rhoPrime = max(lowRho - epsilon/4, 0)\n",
    "    rvi = mdptoolbox.mdp.RelativeValueIteration(transitions, Wrho, epsilon/8)\n",
    "    rvi.run()\n",
    "    reward = rvi.average_reward\n",
    "    policy = rvi.policy\n",
    "    if reward > 0:\n",
    "        lowRho = rho\n",
    "    else:\n",
    "        highRho = rho\n",
    "print('alpha: ', alpha, 'upper bound reward', rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha:  0.45\n",
    "processing state 0\n",
    "alpha:  0.45 lower bound reward: 0.5507888793945312\n",
    "alpha:  0.45 upper bound reward 0.6507766723632813"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
