{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import environmentv6 as e\n",
    "import mdptoolbox\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import progressbar as pb\n",
    "import scipy.sparse as ss\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=ss.SparseEfficiencyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "alpha = 0.45\n",
    "gamma = 0.5\n",
    "T = 75\n",
    "epsilon = 10e-5\n",
    "\n",
    "# game\n",
    "action_count = 4\n",
    "adopt = 0; override = 1; mine = 2; match = 3\n",
    "\n",
    "# fork params\n",
    "fork_count = 3\n",
    "irrelevant = 0; relevant = 1; active = 2;\n",
    "\n",
    "state_count = (T+1) * (T+1) * 3\n",
    "\n",
    "# mapping utils\n",
    "state_mapping = {}\n",
    "states = []\n",
    "count = 0\n",
    "for a in range(T+1):\n",
    "    for h in range(T+1):\n",
    "        for fork in range(fork_count):\n",
    "            state_mapping[(a, h, fork)] = count\n",
    "            states.append((a, h, fork))\n",
    "            count += 1\n",
    "\n",
    "# initialize matrices\n",
    "transitions = []; rewards = []\n",
    "for _ in range(action_count):\n",
    "    transitions.append(ss.csr_matrix(np.zeros(shape=(state_count, state_count))))\n",
    "    rewards.append(ss.csr_matrix(np.zeros(shape=(state_count, state_count))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mining_cost = 0.5\n",
    "\n",
    "# populate matrices\n",
    "for state_index in range(state_count):\n",
    "    a, h, fork = states[state_index]\n",
    "\n",
    "    # adopt\n",
    "    transitions[adopt][state_index, state_mapping[0, 0, irrelevant]] = 1\n",
    "\n",
    "    # override\n",
    "    if a > h:\n",
    "        transitions[override][state_index, state_mapping[a-h-1, 0, irrelevant]] = 1\n",
    "        rewards[override][state_index, state_mapping[a-h-1, 0, irrelevant]] = h + 1\n",
    "    else:\n",
    "        transitions[override][state_index, 0] = 1\n",
    "        rewards[override][state_index, 0] = -10000\n",
    "\n",
    "    # mine \n",
    "    if (fork != active) and (a < T) and (h < T):\n",
    "        transitions[mine][state_index, state_mapping[a+1, h, irrelevant]] = alpha\n",
    "        transitions[mine][state_index, state_mapping[a, h+1, relevant]] = (1 - alpha) \n",
    "        rewards[mine][state_index, state_mapping[a+1, h, irrelevant]] = -1 * alpha * mining_cost\n",
    "        rewards[mine][state_index, state_mapping[a, h+1, relevant]] = -1 * alpha * mining_cost        \n",
    "    elif (fork == active) and (a > h) and (h > 0) and (a < T) and (h < T):\n",
    "        transitions[mine][state_index, state_mapping[a+1, h, active]] = alpha\n",
    "        transitions[mine][state_index, state_mapping[a-h, 1, relevant]] = (1 - alpha) * gamma\n",
    "        transitions[mine][state_index, state_mapping[a, h+1, relevant]] = (1 - alpha) * (1 - gamma)\n",
    "        rewards[mine][state_index, state_mapping[a+1, h, active]] = -1 * alpha * mining_cost\n",
    "        rewards[mine][state_index, state_mapping[a-h, 1, relevant]] = h - alpha * mining_cost\n",
    "        rewards[mine][state_index, state_mapping[a, h+1, relevant]] = -1 * alpha * mining_cost\n",
    "    else:\n",
    "        transitions[mine][state_index, 0] = 1\n",
    "        rewards[mine][state_index, 0] = -10000\n",
    "        \n",
    "    # match \n",
    "    if (fork == relevant) and (a >= h) and (h > 0) and (a < T) and (h < T):\n",
    "        transitions[match][state_index, state_mapping[a+1, h, active]] = alpha\n",
    "        transitions[match][state_index, state_mapping[a-h, 1, relevant]] = (1 - alpha) * gamma\n",
    "        transitions[match][state_index, state_mapping[a, h+1, relevant]] = (1 - alpha) * (1 - gamma)\n",
    "        rewards[match][state_index, state_mapping[a+1, h, active]] = -1 * alpha * mining_cost\n",
    "        rewards[match][state_index, state_mapping[a-h, 1, relevant]] = h - alpha * mining_cost\n",
    "        rewards[match][state_index, state_mapping[a, h+1, relevant]] = -1 * alpha * mining_cost\n",
    "    else:\n",
    "        transitions[match][state_index, 0] = 1\n",
    "        rewards[match][state_index, 0] = -10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wwa & aaa & aaa & aaa & aaa & aaa & aaa & aaa & aaa & \\\\ \n",
      "wwo & wma & wwa & aaa & aaa & aaa & aaa & aaa & aaa & \\\\ \n",
      "wwo & wmw & wma & wwa & aaa & aaa & aaa & aaa & aaa & \\\\ \n",
      "wwo & wmw & wmw & wma & wwa & wwa & aaa & aaa & aaa & \\\\ \n",
      "wwo & wmw & wmw & omw & wma & wwa & wwa & aaa & aaa & \\\\ \n",
      "wwo & wmw & wmw & wmw & omw & wma & wwa & wwa & aaa & \\\\ \n",
      "wwo & wmw & wmw & wmw & wmw & omw & wma & wwa & wwa & \\\\ \n",
      "wwo & wmw & wmw & wmw & wmw & wmw & ooo & wma & wwa & \\\\ \n",
      "wwo & wmw & wmw & wmw & wmw & wmw & wmw & ooo & wma & \\\\ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rvi = mdptoolbox.mdp.RelativeValueIteration(transitions, rewards, epsilon/8)\n",
    "rvi.run()\n",
    "policy = rvi.policy\n",
    "processPolicy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2, 2, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[2, 2, 1],\n",
       "        [2, 3, 0],\n",
       "        [2, 2, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[2, 2, 1],\n",
       "        [2, 3, 2],\n",
       "        [2, 3, 0],\n",
       "        [2, 2, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[2, 2, 1],\n",
       "        [2, 3, 2],\n",
       "        [1, 3, 2],\n",
       "        [2, 3, 0],\n",
       "        [2, 2, 0],\n",
       "        [2, 2, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[2, 2, 1],\n",
       "        [2, 3, 2],\n",
       "        [2, 3, 2],\n",
       "        [1, 1, 1],\n",
       "        [2, 3, 0],\n",
       "        [2, 2, 0],\n",
       "        [2, 2, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[2, 2, 1],\n",
       "        [2, 3, 2],\n",
       "        [2, 3, 2],\n",
       "        [2, 3, 2],\n",
       "        [1, 1, 1],\n",
       "        [2, 3, 0],\n",
       "        [2, 2, 0],\n",
       "        [2, 2, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[2, 2, 1],\n",
       "        [2, 3, 2],\n",
       "        [2, 3, 2],\n",
       "        [2, 3, 2],\n",
       "        [2, 3, 2],\n",
       "        [1, 1, 1],\n",
       "        [2, 3, 0],\n",
       "        [2, 2, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[2, 2, 1],\n",
       "        [2, 3, 2],\n",
       "        [2, 3, 2],\n",
       "        [2, 3, 2],\n",
       "        [2, 3, 2],\n",
       "        [2, 3, 2],\n",
       "        [1, 1, 1],\n",
       "        [2, 3, 0],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [0, 0, 0]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(policy, (9,9,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processPolicy(policy):\n",
    "    results = ''\n",
    "    for a in range(9):\n",
    "        for h in range(9):\n",
    "            for fork in range(3):\n",
    "                state_index = state_mapping[(a, h, fork)]\n",
    "                action = policy[state_index]\n",
    "                if action == 0:\n",
    "                    results += 'a'\n",
    "                elif action == 1:\n",
    "                    results += 'o'\n",
    "                elif action == 2:\n",
    "                    results += 'w'\n",
    "                elif action == 3:\n",
    "                    results += 'm'\n",
    "                else:\n",
    "                    print('here')\n",
    "            results += ' & '\n",
    "        results += '\\\\\\\\ \\n'\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm1_policy = np.asarray([\n",
    "[2, 0, 9, 9, 9, 9, 9, 9, 9],\n",
    "[2, 0, 9, 9, 9, 9, 9, 9, 9],\n",
    "[2, 1, 0, 9, 9, 9, 9, 9, 9], \n",
    "[2, 2, 1, 0, 9, 9, 9, 9, 9],\n",
    "[2, 2, 2, 1, 0, 9, 9, 9, 9],\n",
    "[2, 2, 2, 2, 1, 0, 9, 9, 9],\n",
    "[2, 2, 2, 2, 2, 1, 0, 9, 9],\n",
    "[2, 2, 2, 2, 2, 2, 1, 0, 9],\n",
    "[1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
    "])\n",
    "\n",
    "honest_policy = np.asarray([\n",
    "[2, 0, 9, 9, 9, 9, 9, 9, 9],\n",
    "[1, 9, 9, 9, 9, 9, 9, 9, 9],\n",
    "[9, 9, 9, 9, 9, 9, 9, 9, 9], \n",
    "[9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
    "[9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
    "[9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
    "[9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
    "[9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
    "[9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
    "])\n",
    "\n",
    "opt_policy = np.reshape(policy, (9,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opt_policy(alpha, T, mining_cost):\n",
    "    for state_index in range(state_count):\n",
    "        a, h = states[state_index]\n",
    "\n",
    "        # adopt transitions\n",
    "        transitions[adopt][state_index, state_mapping[0, 0]] = 1\n",
    "\n",
    "        # override\n",
    "        if a > h:\n",
    "            transitions[override][state_index, state_mapping[a-h-1, 0]] = 1\n",
    "            rewards[override][state_index, state_mapping[a-h-1, 0]] = h + 1\n",
    "        else:\n",
    "            transitions[override][state_index, 0] = 1\n",
    "            rewards[override][state_index, 0] = -10000\n",
    "\n",
    "        # mine transitions\n",
    "        if (a < T) and (h < T):\n",
    "            transitions[mine][state_index, state_mapping[a+1, h]] = alpha\n",
    "            transitions[mine][state_index, state_mapping[a, h+1]] = (1 - alpha) \n",
    "            rewards[mine][state_index, state_mapping[a+1, h]] = -1 * alpha * mining_cost\n",
    "            rewards[mine][state_index, state_mapping[a, h+1]] = -1 * alpha * mining_cost        \n",
    "        else:\n",
    "            transitions[mine][state_index, 0] = 1\n",
    "            rewards[mine][state_index, 0] = -10000\n",
    "        \n",
    "        rvi = mdptoolbox.mdp.RelativeValueIteration(transitions, rewards, epsilon/8)\n",
    "        rvi.run()\n",
    "        return np.reshape(rvi.policy, (T+1, T+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 2, 2, 0, 0, 0, 0, 0, 0],\n",
       "       [2, 1, 2, 2, 2, 0, 0, 0, 0],\n",
       "       [2, 2, 1, 2, 2, 2, 0, 0, 0],\n",
       "       [2, 2, 2, 1, 2, 2, 2, 0, 0],\n",
       "       [2, 2, 2, 2, 1, 2, 2, 2, 0],\n",
       "       [2, 2, 2, 2, 2, 1, 2, 2, 0],\n",
       "       [2, 2, 2, 2, 2, 2, 1, 2, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_opt_policy(alpha=0.4, T=8, mining_cost=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1000000 of 1000000) |##############| Elapsed Time: 0:00:28 Time:  0:00:28\n"
     ]
    }
   ],
   "source": [
    "# simulation\n",
    "length = int(1e6)\n",
    "alpha = 0.4\n",
    "T = 8\n",
    "mining_cost = 0.5\n",
    "env = e.Environment(alpha, T, mining_cost)\n",
    "\n",
    "# simulation\n",
    "bar = pb.ProgressBar()\n",
    "_ = env.reset()\n",
    "current_reward = 0\n",
    "for _ in bar(range(length)):\n",
    "    a, h = env.current_state\n",
    "    action = opt_policy[(a,h)]\n",
    "    _, reward = env.takeAction(action)\n",
    "    current_reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101211.00000130651 0.10121100000130652\n"
     ]
    }
   ],
   "source": [
    "# opt\n",
    "print(current_reward, current_reward / length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54266.60000058278 0.05426660000058278\n"
     ]
    }
   ],
   "source": [
    "# sm1\n",
    "print(current_reward, current_reward / length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100698.00000089758 0.10069800000089758\n"
     ]
    }
   ],
   "source": [
    "# honest\n",
    "print(current_reward, current_reward / length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
