{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdptoolbox\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as ss\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=ss.SparseEfficiencyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "alpha = 0.4\n",
    "T = 9\n",
    "state_count = (T+1) * (T+1)\n",
    "epsilon = None\n",
    "\n",
    "# game\n",
    "action_count = 3\n",
    "adopt = 0; override = 1; mine = 2\n",
    "\n",
    "# mapping utils\n",
    "state_mapping = {}\n",
    "states = []\n",
    "count = 0\n",
    "for a in range(T+1):\n",
    "    for h in range(T+1):\n",
    "            state_mapping[(a, h)] = count\n",
    "            states.append((a, h))\n",
    "            count += 1\n",
    "\n",
    "# initialize matrices\n",
    "transitions = []; reward_selfish = []; reward_honest = []\n",
    "for _ in range(action_count):\n",
    "    transitions.append(ss.csr_matrix(np.zeros(shape=(state_count, state_count))))\n",
    "    reward_selfish.append(ss.csr_matrix(np.zeros(shape=(state_count, state_count))))\n",
    "    reward_honest.append(ss.csr_matrix(np.zeros(shape=(state_count, state_count))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state_index in range(state_count):\n",
    "    a, h = states[state_index]\n",
    "\n",
    "    # adopt transitions\n",
    "    transitions[adopt][state_index, state_mapping[0, 0]] = 1\n",
    "    # adopt rewards\n",
    "    reward_honest[adopt][state_index, state_mapping[0, 0]] = h\n",
    "\n",
    "    # override\n",
    "    if a > h:\n",
    "        transitions[override][state_index, state_mapping[a-h-1, 0]] = 1\n",
    "        reward_selfish[override][state_index, state_mapping[a-h-1, 0]] = h+1\n",
    "    else:\n",
    "        transitions[override][state_index, 0] = 1\n",
    "        reward_honest[override][state_index, 0] = 10000\n",
    "\n",
    "    # mine transitions\n",
    "    if (a < self.T) and (h < self.T):\n",
    "        transitions[mine][state_index, self.state_mapping[a+1, h]] = self.alpha\n",
    "        self.transitions[self.wait][state_index, self.state_mapping[a, h+1]] = 1 - self.alpha\n",
    "    else:\n",
    "        self.transitions[self.wait][state_index, 0] = 1\n",
    "        self.reward_honest[self.wait][state_index, 0] = 10000\n",
    "\n",
    "    def getRhoBounds(self):\n",
    "        low = 0; high = 1\n",
    "        while (high - low) > self.epsilon / 8:\n",
    "            rho = (low + high) / 2\n",
    "            print(low, high, rho)\n",
    "            total_reward = []\n",
    "            for i in range(self.action_count):\n",
    "                total_reward.append((1-rho)*self.reward_selfish[i] - rho*self.reward_honest[i])\n",
    "            rvi = mdptoolbox.mdp.RelativeValueIteration(self.transitions, total_reward, self.epsilon/8)\n",
    "            rvi.run()\n",
    "            if rvi.average_reward > 0:\n",
    "                low = rho\n",
    "            else:\n",
    "                high = rho\n",
    "        opt_policy = rvi.policy\n",
    "        print('alpha: ', self.alpha, 'lower bound reward:', rho)\n",
    "        \n",
    "        # ql = mdptoolbox.mdp.QLearning(self.transitions, total_reward, discount=1, n_iter=100000)\n",
    "        # ql.run()\n",
    "        # self.processPolicy(ql.policy)\n",
    "        \n",
    "        # vi = mdptoolbox.mdp.ValueIteration(self.transitions, total_reward, discount=1, epsilon=self.epsilon/8)\n",
    "        # vi.run()\n",
    "        # self.processPolicy(vi.policy)\n",
    "        \n",
    "        # pi = mdptoolbox.mdp.PolicyIteration(self.transitions, total_reward, discount=0.99, eval_type=1)\n",
    "        # pi.run()\n",
    "        # self.processPolicy(pi.policy)\n",
    "        \n",
    "        self.processPolicy(opt_policy)\n",
    "        \n",
    "    def processPolicy(self, policy):\n",
    "        results = ''\n",
    "        for a in range(9):\n",
    "            results += '{} & '.format(a)\n",
    "            for h in range(9):\n",
    "                state_index = self.state_mapping[(a, h)]\n",
    "                action = policy[state_index]\n",
    "                assert(action in [0, 1, 2])\n",
    "                if action == 0:\n",
    "                    results += 'a'\n",
    "                elif action == 1:\n",
    "                    results += 'o'\n",
    "                else:\n",
    "                    results += 'w'\n",
    "                results += ' & '\n",
    "            results = results[:-2]\n",
    "            results += '\\\\\\\\ \\n'\n",
    "        print(results)\n",
    "\n",
    "def main():\n",
    "    for alpha in [0.4]:\n",
    "        print(alpha)\n",
    "        mdp = SelfishMDP(alpha=alpha, T=9, epsilon=10e-5)\n",
    "        mdp.getRhoBounds()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
